{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import os, sys\n",
    "sys.path.append(r'/elo_bench')\n",
    "from datamodel import QuestionCollection\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"convert gemini answers to the designed format csv\"\"\"\n",
    "if False:\n",
    "    gemini_answers = pd.read_json(r'./gemini_pro_answer_20573_20231228.jsonl', lines=True)\n",
    "\n",
    "    # take 'question' as row index, and 'answer' as cell of ['question', 'gemini']\n",
    "    gemini_answers = gemini_answers.set_index('question')\n",
    "    gemini_answers = gemini_answers.rename(columns={'answer': 'gemini'})\n",
    "    gemini_answers.to_csv(r'./q_and_as gemini.csv')\n",
    "    gemini_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing information in ./q_and_as 2a100.csv...\n",
      "21118 answers for lmsys/vicuna-7b-v1.5\n",
      "21083 answers for gpt-4-turbo\n",
      "21090 answers for gpt-35-turbo\n",
      "21191 answers for lmsys/vicuna-13b-v1.5\n",
      "21114 answers for lmsys/vicuna-33b-v1.3\n",
      "21115 answers for meta-llama/Llama-2-7b-chat-hf\n",
      "21119 answers for meta-llama/Llama-2-13b-chat-hf\n",
      "105 answers for huggyllama/llama-13b\n",
      "14577 answers for meta-llama/Llama-2-70b-chat-hf\n",
      "105 answers for tiiuae/falcon-7b-instruct\n",
      "105 answers for tiiuae/falcon-40b-instruct\n",
      "105 answers for mosaicml/mpt-7b-chat\n",
      "105 answers for WizardLM/WizardLM-13B-V1.2\n",
      "822 answers for Xwin-LM/Xwin-LM-7B-V0.1\n",
      "854 answers for chavinlo/alpaca-13b\n",
      "853 answers for WizardLM/WizardLM-7B-V1.0\n",
      "809 answers for chavinlo/alpaca-native\n",
      "824 answers for Xwin-LM/Xwin-LM-13B-V0.1\n",
      "105 answers for mosaicml/mpt-30b-chat\n",
      "105 answers for huggyllama/llama-7b\n",
      "105 answers for HuggingFaceH4/zephyr-7b-beta\n",
      "20 answers for huggyllama/llama-30b\n",
      "Printing information in ./q_and_as 0118 1503.csv...\n",
      "21963 answers for Xwin-LM/Xwin-LM-13B-V0.1\n",
      "21963 answers for Xwin-LM/Xwin-LM-7B-V0.1\n",
      "21963 answers for WizardLM/WizardLM-7B-V1.0\n",
      "21963 answers for WizardLM/WizardLM-13B-V1.2\n",
      "21963 answers for mosaicml/mpt-7b-chat\n",
      "7747 answers for mosaicml/mpt-30b-chat\n",
      "8 answers for HuggingFaceH4/zephyr-7b-beta\n",
      "Printing information in ./q_and_as 0118 1673.csv...\n",
      "10309 answers for huggyllama/llama-7b\n",
      "10309 answers for huggyllama/llama-13b\n",
      "10308 answers for huggyllama/llama-30b\n",
      "10309 answers for chavinlo/alpaca-native\n",
      "10309 answers for chavinlo/alpaca-13b\n",
      "4243 answers for mosaicml/mpt-30b-chat\n",
      "Printing information in ./q_and_as 0116 1673.csv...\n",
      "21963 answers for huggyllama/llama-7b\n",
      "21963 answers for huggyllama/llama-13b\n",
      "21960 answers for huggyllama/llama-30b\n",
      "21963 answers for chavinlo/alpaca-native\n",
      "21963 answers for chavinlo/alpaca-13b\n",
      "Printing information in ./q_and_as 1691...\n",
      "21963 answers for HuggingFaceH4/zephyr-7b-beta\n",
      "21963 answers for tiiuae/falcon-7b-instruct\n",
      "Printing information in ./q_and_as gemini.csv...\n",
      "20573 answers for gemini\n",
      "Printing information in ./q_and_as 0118 2A100 split.csv...\n",
      "18534 answers for tiiuae/falcon-40b-instruct\n"
     ]
    }
   ],
   "source": [
    "\"\"\"list csv splits models name with answers count and percentage\"\"\"\n",
    "if True:\n",
    "    split_csv_files = [\n",
    "        r'./q_and_as 2a100.csv',\n",
    "        r'./q_and_as 0118 1503 new.csv',\n",
    "        r'./q_and_as 0118 1673 new.csv',\n",
    "        r'./q_and_as 0116 1673.csv',\n",
    "        r'./q_and_as 1691', # √\n",
    "        r'./q_and_as gemini.csv', # √\n",
    "        r'./q_and_as 0118 2a100 split new.csv'\n",
    "    ]\n",
    "\n",
    "    for split_csv in split_csv_files:\n",
    "        print(f'Printing information in {split_csv}...')\n",
    "        answer_split = pd.read_csv(split_csv, engine='python', keep_default_na=False, na_values=['NaN', 'NULL'])\n",
    "        skip_columns = ['Unnamed: 0', 'question']\n",
    "        models = [x for x in answer_split.columns.tolist() if not x in (skip_columns)]\n",
    "        for model in models:\n",
    "            print(f'{len(answer_split[model][~answer_split[model].isna()].tolist())} answers for {model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] Removed 48 repeat questions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20772 questions.\n",
      "Printing information in ./q_and_as 2a100.csv...\n",
      "lmsys/vicuna-7b-v1.5\n",
      "gpt-4-turbo\n",
      "gpt-35-turbo\n",
      "lmsys/vicuna-13b-v1.5\n",
      "lmsys/vicuna-33b-v1.3\n",
      "meta-llama/Llama-2-7b-chat-hf\n",
      "meta-llama/Llama-2-13b-chat-hf\n",
      "huggyllama/llama-13b\n",
      "meta-llama/Llama-2-70b-chat-hf\n",
      "tiiuae/falcon-7b-instruct\n",
      "tiiuae/falcon-40b-instruct\n",
      "mosaicml/mpt-7b-chat\n",
      "WizardLM/WizardLM-13B-V1.2\n",
      "Xwin-LM/Xwin-LM-7B-V0.1\n",
      "chavinlo/alpaca-13b\n",
      "WizardLM/WizardLM-7B-V1.0\n",
      "chavinlo/alpaca-native\n",
      "Xwin-LM/Xwin-LM-13B-V0.1\n",
      "mosaicml/mpt-30b-chat\n",
      "huggyllama/llama-7b\n",
      "HuggingFaceH4/zephyr-7b-beta\n",
      "huggyllama/llama-30b\n",
      "Printing information in ./q_and_as 0118 1503.csv...\n",
      "Xwin-LM/Xwin-LM-13B-V0.1\n",
      "Xwin-LM/Xwin-LM-7B-V0.1\n",
      "WizardLM/WizardLM-7B-V1.0\n",
      "WizardLM/WizardLM-13B-V1.2\n",
      "mosaicml/mpt-7b-chat\n",
      "mosaicml/mpt-30b-chat\n",
      "HuggingFaceH4/zephyr-7b-beta\n",
      "Printing information in ./q_and_as 0118 1673.csv...\n",
      "huggyllama/llama-7b\n",
      "huggyllama/llama-13b\n",
      "huggyllama/llama-30b\n",
      "chavinlo/alpaca-native\n",
      "chavinlo/alpaca-13b\n",
      "mosaicml/mpt-30b-chat\n",
      "Printing information in ./q_and_as 0116 1673.csv...\n",
      "huggyllama/llama-7b\n",
      "huggyllama/llama-13b\n",
      "huggyllama/llama-30b\n",
      "chavinlo/alpaca-native\n",
      "chavinlo/alpaca-13b\n",
      "Printing information in ./q_and_as 1691...\n",
      "HuggingFaceH4/zephyr-7b-beta\n",
      "tiiuae/falcon-7b-instruct\n",
      "Printing information in ./q_and_as gemini.csv...\n",
      "gemini\n",
      "Printing information in ./q_and_as 0118 2A100 split.csv...\n",
      "tiiuae/falcon-40b-instruct\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Merge the answers for dataset\"\"\"\n",
    "if True:\n",
    "    dataset_dir = r'/elo_bench/data/google_quora_alpaca_sharegpt_chatlm_clean_20772'\n",
    "    questions = QuestionCollection.read_csv(Path(dataset_dir)/'questions.csv').questions\n",
    "    question_set = set(questions)\n",
    "\n",
    "    print(f'{len(questions)} questions.')\n",
    "\n",
    "    split_csv_files = [\n",
    "        r'./q_and_as 2a100.csv',\n",
    "        r'./q_and_as 0118 1503.csv',\n",
    "        r'./q_and_as 0118 1673.csv',\n",
    "        r'./q_and_as 0116 1673.csv',\n",
    "        r'./q_and_as 1691', # √\n",
    "        r'./q_and_as gemini.csv', # √\n",
    "        r'./q_and_as 0118 2A100 split.csv',\n",
    "    ]\n",
    "\n",
    "    q_and_as_dict = defaultdict(lambda: defaultdict(str)) #  a_and_as_dict['question']['model']=ans\n",
    "    for split_csv in split_csv_files:\n",
    "        print(f'Printing information in {split_csv}...')\n",
    "        answer_split = pd.read_csv(split_csv, keep_default_na=False, na_values=['NaN'], engine='python')\n",
    "        # answer_splits.append(answer_split)\n",
    "        skip_columns = ['Unnamed: 0', 'question']\n",
    "        models = [str(x) for x in answer_split.columns.tolist() if not x in (skip_columns)]\n",
    "        for model in models:\n",
    "            print(model)\n",
    "            for index, row in answer_split.iterrows():\n",
    "                question = row['question']\n",
    "                model_answer = row[model]\n",
    "                # print(f'{question}:{model_answer}')\n",
    "                if not pd.isna(model_answer):\n",
    "                    q_and_as_dict[question][model] = model_answer\n",
    "\n",
    "    reformat_matrix = []\n",
    "    for question_key, model_answers in q_and_as_dict.items():\n",
    "        if question_key not in question_set:\n",
    "            continue\n",
    "        item = {\n",
    "            'question': question_key,\n",
    "        }\n",
    "        for model_name, ans in model_answers.items():\n",
    "            item[model_name] = ans\n",
    "        \n",
    "        reformat_matrix.append(item)\n",
    "        \n",
    "        \n",
    "    df = pd.DataFrame.from_dict(reformat_matrix)\n",
    "    df\n",
    "    df.to_csv(r'./merge.csv', na_rep='NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing information in ./merge.csv...\n",
      "19980 answers for lmsys/vicuna-7b-v1.5\n",
      "19947 answers for gpt-4-turbo\n",
      "19952 answers for gpt-35-turbo\n",
      "19981 answers for lmsys/vicuna-13b-v1.5\n",
      "19976 answers for lmsys/vicuna-33b-v1.3\n",
      "19978 answers for meta-llama/Llama-2-7b-chat-hf\n",
      "19982 answers for meta-llama/Llama-2-13b-chat-hf\n",
      "20772 answers for Xwin-LM/Xwin-LM-13B-V0.1\n",
      "20772 answers for Xwin-LM/Xwin-LM-7B-V0.1\n",
      "20772 answers for WizardLM/WizardLM-7B-V1.0\n",
      "20772 answers for WizardLM/WizardLM-13B-V1.2\n",
      "20772 answers for mosaicml/mpt-7b-chat\n",
      "20772 answers for huggyllama/llama-7b\n",
      "20772 answers for huggyllama/llama-13b\n",
      "20771 answers for huggyllama/llama-30b\n",
      "20772 answers for chavinlo/alpaca-native\n",
      "20772 answers for chavinlo/alpaca-13b\n",
      "20772 answers for HuggingFaceH4/zephyr-7b-beta\n",
      "20772 answers for tiiuae/falcon-7b-instruct\n",
      "19795 answers for gemini\n",
      "18545 answers for tiiuae/falcon-40b-instruct\n",
      "14301 answers for meta-llama/Llama-2-70b-chat-hf\n",
      "12028 answers for mosaicml/mpt-30b-chat\n"
     ]
    }
   ],
   "source": [
    "\"\"\"list csv splits models name with answers count and percentage\"\"\"\n",
    "if True:\n",
    "    split_csv_files = [\n",
    "        r'./merge.csv',\n",
    "    ]\n",
    "\n",
    "    for split_csv in split_csv_files:\n",
    "        print(f'Printing information in {split_csv}...')\n",
    "        answer_split = pd.read_csv(split_csv, engine='python', keep_default_na=False, na_values=['NaN', 'NULL'])\n",
    "        skip_columns = ['Unnamed: 0', 'question']\n",
    "        models = [x for x in answer_split.columns.tolist() if not x in (skip_columns)]\n",
    "        for model in models:\n",
    "            model_answers = answer_split[model]\n",
    "            model_answers = model_answers[~model_answers.isna()]\n",
    "            print(f'{len(model_answers.tolist())} answers for {model}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
